{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "domestic-tyler",
   "metadata": {},
   "source": [
    "# Time Series Forecasting on NYC_Taxi\n",
    "\n",
    "w MLFlow\n",
    "\n",
    "- Objectives\n",
    "  - Leverage ML FLow to build some time series models\n",
    "\n",
    "- Simple Forecast of aggregate daily data to start\n",
    "- Later will need to look at splitting out the datasets into different spots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "associate-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "packages=\"\"\"io.delta:delta-core_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.2.0\"\"\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages io.delta:delta-core_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.2.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "corporate-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(\"spark.executor.cores\", 4) \\\n",
    "            .config(\"spark.executor.memory\", \"4g\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .config(\"spark.master\", \"spark://spark-master:7077\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", os.environ['MINIO_ACCESS_KEY']) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ['MINIO_SECRET_KEY']) \\\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"minio:9000\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "            .config(\"spark.hadoop.metastore.catalog.default\", \"hive\") \\\n",
    "            .config(\"spark.sql.warehouse.dir\", \"s3a://storage/warehouse\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"50\") \\\n",
    "            .config(\"spark.hive.metastore.uris\", \"thrift://192.168.64.4:9083\") \\\n",
    "            .appName(\"Jupyter Time Series\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "executive-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning settings spark.sql.shuffle.partitions to match with core settings\n",
    "# not tuned for file size yet\n",
    "\n",
    "# set to cores to increase the efficiency\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism*4)\n",
    "\n",
    "## faster pandas data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "formal-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading out the dataset\n",
    "nyc_taxi_dataset = spark.sql(\"SELECT * FROM processed.nyc_taxi_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "subject-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data_source: string (nullable = true)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- rate_code_id: string (nullable = true)\n",
      " |-- pickup_longitude: float (nullable = true)\n",
      " |-- pickup_latitude: float (nullable = true)\n",
      " |-- dropoff_longitude: float (nullable = true)\n",
      " |-- dropoff_latitude: float (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: float (nullable = true)\n",
      " |-- fare_amount: float (nullable = true)\n",
      " |-- extra: float (nullable = true)\n",
      " |-- mta_tax: float (nullable = true)\n",
      " |-- tip_amount: float (nullable = true)\n",
      " |-- tolls_amount: float (nullable = true)\n",
      " |-- ehail_fee: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- trip_type: integer (nullable = true)\n",
      " |-- improvement_surcharge: float (nullable = true)\n",
      " |-- rate_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nyc_taxi_dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accredited-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "republican-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets do some dailies to start\n",
    "daily_totals = (\n",
    "    nyc_taxi_dataset\n",
    "        .withColumn(\"pickup_date\", F.to_date(\"pickup_datetime\"))\n",
    "        .select(\"pickup_date\", \"total_amount\")\n",
    "        .groupBy(\"pickup_date\")\n",
    "        .agg(\n",
    "            F.count(\"total_amount\").alias(\"total_rides\"),\n",
    "            F.sum(\"total_amount\").alias(\"total_takings\")\n",
    "        )\n",
    "        .sort(\"pickup_date\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "anticipated-mustang",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- total_rides: long (nullable = false)\n",
      " |-- total_takings: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_totals.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "guilty-livestock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(pickup_date=datetime.date(2013, 8, 1), total_rides=307272, total_takings=4567414.341715002),\n",
       " Row(pickup_date=datetime.date(2013, 8, 2), total_rides=236761, total_takings=3534615.589567093),\n",
       " Row(pickup_date=datetime.date(2013, 8, 3), total_rides=226554, total_takings=3182267.26641283),\n",
       " Row(pickup_date=datetime.date(2013, 8, 4), total_rides=202310, total_takings=3073608.618903598),\n",
       " Row(pickup_date=datetime.date(2013, 8, 5), total_rides=406487, total_takings=6657712.825942375),\n",
       " Row(pickup_date=datetime.date(2013, 8, 6), total_rides=448272, total_takings=6576702.611193515),\n",
       " Row(pickup_date=datetime.date(2013, 8, 7), total_rides=465702, total_takings=6925976.081809193),\n",
       " Row(pickup_date=datetime.date(2013, 8, 8), total_rides=437214, total_takings=6515832.371952146),\n",
       " Row(pickup_date=datetime.date(2013, 8, 9), total_rides=489393, total_takings=7254375.474818595),\n",
       " Row(pickup_date=datetime.date(2013, 8, 10), total_rides=445096, total_takings=6392653.797500223)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_totals.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "previous-donor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_totals.filter(\"pickup_date < '2014-07-01'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chief-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "### format Aug 2013 data\n",
    "daily_rides_cut = (daily_totals.select(F.col(\"pickup_date\").alias(\"ds\"),\n",
    "                   F.col(\"total_rides\").alias(\"y\")).filter(\"pickup_date < '2014-07-01'\").toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-windsor",
   "metadata": {},
   "source": [
    "## Forecasting the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "silent-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials for storing our model artifacts\n",
    "# mlflow needs these to be set whenever it is being called\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = os.environ.get('MINIO_ACCESS_KEY')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = os.environ.get('MINIO_SECRET_KEY')\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = 'http://minio:9000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "organized-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prophet\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.diagnostics import performance_metrics\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-article",
   "metadata": {},
   "source": [
    "There was an error in the hostname resolution hence switch to ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "occupational-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlflow.delete_experiment(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "senior-muslim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current tracking uri: http://192.168.64.21:5000/\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://192.168.64.21:5000/\")\n",
    "tracking_uri = mlflow.get_tracking_uri()\n",
    "print(\"Current tracking uri: {}\".format(tracking_uri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "outdoor-rocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already_created\n"
     ]
    }
   ],
   "source": [
    "### Quick test on creating experiments\n",
    "from mlflow.exceptions import RestException\n",
    "\n",
    "try:\n",
    "    mlflow.create_experiment(\n",
    "        name='taxi_daily_forecast'\n",
    "    )\n",
    "except RestException:\n",
    "    print('already_created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "piano-assault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://mlflow/15'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = mlflow.get_experiment(15)\n",
    "experiment.artifact_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bulgarian-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an evaluation function\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    mae = mean_absolute_error(actual, pred)\n",
    "    r2 = r2_score(actual, pred)\n",
    "    return rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "after-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save models to mlflow we need to write a python wrapper \n",
    "# to make sure that it performs as mlflow expects\n",
    "import mlflow.pyfunc\n",
    "\n",
    "class ProphetModel(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        \n",
    "        self.model = model\n",
    "        super().__init__()\n",
    "        \n",
    "    def load_context(self, context):\n",
    "        from prophet import Prophet\n",
    "        return\n",
    "    \n",
    "    def predict(self, context, model_input):\n",
    "        future = self.model.make_future_dataframe(periods=model_input['periods'][0])\n",
    "        return self.model.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-gothic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
      "INFO:prophet:Making 42 forecasts with cutoffs between 2013-09-02 00:00:00 and 2014-06-16 00:00:00\n"
     ]
    }
   ],
   "source": [
    "rolling_window = 0.1\n",
    "\n",
    "conda_env = {\n",
    "    'channels': ['conda-forge'],\n",
    "    'dependencies': [{\n",
    "        'pip': [\n",
    "            'prophet=={0}'.format(prophet.__version__)\n",
    "        ]\n",
    "    }],\n",
    "    \"name\": \"prophetenv\"\n",
    "}\n",
    "\n",
    "with mlflow.start_run(experiment_id=15):\n",
    "    m = prophet.Prophet(daily_seasonality=True)\n",
    "    m.fit(daily_rides_cut)\n",
    "    \n",
    "    # cross validation is the thingy that is generating our different train sets\n",
    "    # tqdm is glitchy with my setup so disabling for now\n",
    "    df_cv = cross_validation(m, initial=\"28 days\", period=\"7 days\", horizon=\"14 days\", \n",
    "                                 disable_tqdm=True)\n",
    "    df_p = performance_metrics(df_cv, rolling_window=rolling_window)\n",
    "    \n",
    "    mlflow.log_param(\"rolling_window\", rolling_window)\n",
    "    mlflow.log_metric(\"rmse\", df_p.loc[0, \"rmse\"])\n",
    "    mlflow.log_metric(\"mae\", df_p.loc[0, \"mae\"])\n",
    "    mlflow.log_metric(\"mape\", df_p.loc[0, \"mape\"])\n",
    "    \n",
    "    print(\"  CV: {}\".format(df_cv.head()))\n",
    "    print(\"  Perf: {}\".format(df_p.head()))\n",
    "    \n",
    "    mlflow.pyfunc.log_model(\"model\", conda_env=conda_env, python_model=ProphetModel(m))\n",
    "    print(\n",
    "            \"Logged model with URI: runs:/{run_id}/model\".format(\n",
    "                run_id=mlflow.active_run().info.run_id\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Plots\n",
    "fig1 = m.plot(forecast)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
